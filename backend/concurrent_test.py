#!/usr/bin/env python3
"""
æ ‡æ³¨ç³»ç»Ÿå¹¶å‘å‹åŠ›æµ‹è¯•
æ¨¡æ‹Ÿå¤šä¸ªç”¨æˆ·åŒæ—¶è¿›è¡Œæ ‡æ³¨æ“ä½œï¼ŒéªŒè¯record_idçš„å”¯ä¸€æ€§å’Œå¹¶å‘å®‰å…¨æ€§
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from pymongo import MongoClient
from db_utils import get_next_annotation_id
import threading
import time
import random
from datetime import datetime
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
MONGO_URI = os.getenv('MONGODB_URI', 'mongodb://172.20.48.1:27017/local')
MONGO_DB = os.getenv('MONGODB_DB', 'local')

def concurrent_annotation_test():
    """å¹¶å‘æ ‡æ³¨æµ‹è¯•"""
    print("=== å¹¶å‘æ ‡æ³¨å‹åŠ›æµ‹è¯• ===")
    
    # æµ‹è¯•å‚æ•°
    THREAD_COUNT = 5  # å¹¶å‘çº¿ç¨‹æ•°
    ANNOTATIONS_PER_THREAD = 10  # æ¯ä¸ªçº¿ç¨‹åˆ›å»ºçš„æ ‡æ³¨æ•°
    
    client = MongoClient(MONGO_URI)
    db = client[MONGO_DB]
    
    # è®°å½•æµ‹è¯•å¼€å§‹å‰çš„æ•°æ®åº“çŠ¶æ€
    initial_count = db.annotations.count_documents({})
    print(f"æµ‹è¯•å¼€å§‹å‰æ•°æ®åº“ä¸­æ ‡æ³¨æ•°: {initial_count}")
    
    # ç”¨äºæ”¶é›†æ‰€æœ‰ç”Ÿæˆçš„record_id
    all_record_ids = []
    lock = threading.Lock()
    
    # ç”¨äºè®°å½•çº¿ç¨‹æ‰§è¡Œæ—¶é—´
    thread_times = {}
    
    def annotate_worker(thread_id):
        """å·¥ä½œçº¿ç¨‹ï¼šæ¨¡æ‹Ÿç”¨æˆ·æ ‡æ³¨æ“ä½œ"""
        start_time = time.time()
        thread_record_ids = []
        
        for i in range(ANNOTATIONS_PER_THREAD):
            try:
                # æ¨¡æ‹Ÿæ ‡æ³¨æ•°æ®
                annotation_data = {
                    'dataset_id': random.randint(1, 3),
                    'image_id': random.randint(1, 100),
                    'expert_id': thread_id,  # ä½¿ç”¨çº¿ç¨‹IDä½œä¸ºä¸“å®¶ID
                    'label_id': random.randint(1, 5),
                    'tip': f'çº¿ç¨‹{thread_id}çš„ç¬¬{i+1}ä¸ªæ ‡æ³¨',
                    'datetime': datetime.now().isoformat()
                }
                
                # è·å–å”¯ä¸€çš„record_id
                record_id = get_next_annotation_id(db)
                annotation_data['record_id'] = record_id
                
                # ä¿å­˜åˆ°æ•°æ®åº“
                result = db.annotations.insert_one(annotation_data)
                
                thread_record_ids.append(record_id)
                print(f"çº¿ç¨‹{thread_id}: æ ‡æ³¨{i+1}, record_id={record_id}, ObjectId={result.inserted_id}")
                
                # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
                time.sleep(random.uniform(0.01, 0.05))
                
            except Exception as e:
                print(f"âŒ çº¿ç¨‹{thread_id}æ ‡æ³¨{i+1}å¤±è´¥: {e}")
        
        end_time = time.time()
        
        # çº¿ç¨‹å®‰å…¨åœ°æ·»åŠ åˆ°å…¨å±€åˆ—è¡¨
        with lock:
            all_record_ids.extend(thread_record_ids)
            thread_times[thread_id] = end_time - start_time
        
        print(f"âœ… çº¿ç¨‹{thread_id}å®Œæˆï¼Œè€—æ—¶{end_time - start_time:.2f}ç§’")
    
    # åˆ›å»ºå¹¶å¯åŠ¨æ‰€æœ‰çº¿ç¨‹
    threads = []
    test_start_time = time.time()
    
    print(f"\nå¯åŠ¨{THREAD_COUNT}ä¸ªå¹¶å‘çº¿ç¨‹ï¼Œæ¯ä¸ªçº¿ç¨‹åˆ›å»º{ANNOTATIONS_PER_THREAD}ä¸ªæ ‡æ³¨...")
    
    for i in range(THREAD_COUNT):
        thread = threading.Thread(target=annotate_worker, args=(i+1,))
        threads.append(thread)
        thread.start()
    
    # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
    for thread in threads:
        thread.join()
    
    test_end_time = time.time()
    
    # åˆ†ææµ‹è¯•ç»“æœ
    print(f"\n=== æµ‹è¯•ç»“æœåˆ†æ ===")
    print(f"æ€»è€—æ—¶: {test_end_time - test_start_time:.2f}ç§’")
    print(f"å¹³å‡çº¿ç¨‹è€—æ—¶: {sum(thread_times.values()) / len(thread_times):.2f}ç§’")
    
    # æ£€æŸ¥record_idå”¯ä¸€æ€§
    total_generated = len(all_record_ids)
    unique_record_ids = set(all_record_ids)
    unique_count = len(unique_record_ids)
    
    print(f"ç”Ÿæˆçš„record_idæ€»æ•°: {total_generated}")
    print(f"å”¯ä¸€record_idæ•°é‡: {unique_count}")
    print(f"record_idèŒƒå›´: {min(all_record_ids)} - {max(all_record_ids)}")
    
    if total_generated == unique_count:
        print("âœ… å¹¶å‘å”¯ä¸€æ€§æµ‹è¯•é€šè¿‡ï¼šæ‰€æœ‰record_idéƒ½æ˜¯å”¯ä¸€çš„")
    else:
        print("âŒ å¹¶å‘å”¯ä¸€æ€§æµ‹è¯•å¤±è´¥ï¼šå‘ç°é‡å¤çš„record_id")
        duplicates = [rid for rid in all_record_ids if all_record_ids.count(rid) > 1]
        print(f"é‡å¤çš„record_id: {set(duplicates)}")
    
    # éªŒè¯æ•°æ®åº“çŠ¶æ€
    final_count = db.annotations.count_documents({})
    expected_count = initial_count + (THREAD_COUNT * ANNOTATIONS_PER_THREAD)
    
    print(f"æµ‹è¯•åæ•°æ®åº“ä¸­æ ‡æ³¨æ•°: {final_count}")
    print(f"é¢„æœŸæ ‡æ³¨æ•°: {expected_count}")
    
    if final_count == expected_count:
        print("âœ… æ•°æ®åº“ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")
    else:
        print(f"âŒ æ•°æ®åº“ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥ï¼šç¼ºå°‘{expected_count - final_count}æ¡è®°å½•")
    
    # æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦æœ‰é‡å¤çš„record_id
    pipeline = [
        {"$group": {"_id": "$record_id", "count": {"$sum": 1}}},
        {"$match": {"count": {"$gt": 1}}}
    ]
    db_duplicates = list(db.annotations.aggregate(pipeline))
    
    if not db_duplicates:
        print("âœ… æ•°æ®åº“record_idå”¯ä¸€æ€§æ£€æŸ¥é€šè¿‡")
    else:
        print(f"âŒ æ•°æ®åº“ä¸­å‘ç°{len(db_duplicates)}ä¸ªé‡å¤çš„record_id")
        for dup in db_duplicates:
            print(f"   record_id {dup['_id']}: {dup['count']} æ¡è®°å½•")
    
    client.close()
    
    if total_generated == unique_count and final_count == expected_count and not db_duplicates:
        print("\nğŸ‰ å¹¶å‘å‹åŠ›æµ‹è¯•å®Œå…¨é€šè¿‡ï¼")
        return True
    else:
        print("\nâŒ å¹¶å‘å‹åŠ›æµ‹è¯•å¤±è´¥ï¼")
        return False

if __name__ == "__main__":
    success = concurrent_annotation_test()
    sys.exit(0 if success else 1)
